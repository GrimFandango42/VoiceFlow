name: VoiceFlow Comprehensive Testing Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run comprehensive tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of tests to run'
        required: true
        default: 'comprehensive'
        type: choice
        options:
          - 'unit'
          - 'integration'
          - 'e2e'
          - 'performance'
          - 'security'
          - 'comprehensive'
      performance_baseline:
        description: 'Update performance baseline'
        required: false
        default: false
        type: boolean

env:
  PYTHON_VERSION: '3.9'
  NODE_VERSION: '18'

jobs:
  # Job 1: Unit Tests (Fast feedback)
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'unit' || github.event.inputs.test_type == 'comprehensive' || github.event.inputs.test_type == ''
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y xvfb
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements_testing.txt
        pip install pytest-cov pytest-xdist pytest-html
    
    - name: Start virtual display
      run: |
        export DISPLAY=:99
        Xvfb :99 -screen 0 1024x768x24 > /dev/null 2>&1 &
    
    - name: Run basic CI tests
      run: |
        python test_basic_ci.py
    
    - name: Run unit tests
      run: |
        python test_orchestrator.py --types unit --parallel --output-dir test_results
      env:
        DISPLAY: ':99'
    
    - name: Upload unit test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: unit-test-results
        path: test_results/
        retention-days: 30
    
    - name: Publish test results
      uses: dorny/test-reporter@v1
      if: always()
      with:
        name: Unit Tests
        path: 'test_results/test_results_*.json'
        reporter: java-junit
        fail-on-error: true

  # Job 2: Integration Tests
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'integration' || github.event.inputs.test_type == 'comprehensive' || github.event.inputs.test_type == ''
    needs: unit-tests
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y portaudio19-dev python3-pyaudio
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements_testing.txt
    
    - name: Run integration tests
      run: |
        python test_orchestrator.py --types integration --output-dir test_results
    
    - name: Upload integration test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: integration-test-results
        path: test_results/
        retention-days: 30

  # Job 3: End-to-End Tests
  e2e-tests:
    name: End-to-End Tests
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'e2e' || github.event.inputs.test_type == 'comprehensive' || github.event.inputs.test_type == ''
    needs: integration-tests
    
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install system dependencies (Ubuntu)
      if: matrix.os == 'ubuntu-latest'
      run: |
        sudo apt-get update
        sudo apt-get install -y portaudio19-dev python3-pyaudio xvfb
    
    - name: Install system dependencies (macOS)
      if: matrix.os == 'macos-latest'
      run: |
        brew install portaudio
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements_testing.txt
    
    - name: Run comprehensive E2E tests
      run: |
        python comprehensive_test_suite.py
      env:
        DISPLAY: ':99'
    
    - name: Start virtual display (Ubuntu)
      if: matrix.os == 'ubuntu-latest'
      run: |
        Xvfb :99 -screen 0 1024x768x24 > /dev/null 2>&1 &
    
    - name: Upload E2E test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: e2e-test-results-${{ matrix.os }}
        path: comprehensive_test_results_*.json
        retention-days: 30

  # Job 4: Performance Tests
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'performance' || github.event.inputs.test_type == 'comprehensive' || github.event.inputs.test_type == ''
    needs: unit-tests
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements_testing.txt
        pip install psutil matplotlib seaborn pandas numpy
    
    - name: Download performance baseline
      continue-on-error: true
      run: |
        # Download baseline from artifacts or repository
        curl -f -o performance_baseline.json https://api.github.com/repos/${{ github.repository }}/releases/latest/assets/performance_baseline.json || echo "No baseline found"
    
    - name: Run performance regression tests
      run: |
        python performance_regression_tests.py
    
    - name: Update performance baseline
      if: github.event.inputs.performance_baseline == 'true' && github.ref == 'refs/heads/main'
      run: |
        # Upload new baseline as release asset
        echo "Updating performance baseline..."
    
    - name: Upload performance test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: performance-test-results
        path: |
          performance_regression_report_*.json
          performance_baseline.json
        retention-days: 90
    
    - name: Comment performance results on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const glob = require('glob');
          
          const reportFiles = glob.sync('performance_regression_report_*.json');
          if (reportFiles.length > 0) {
            const report = JSON.parse(fs.readFileSync(reportFiles[0], 'utf8'));
            
            const comment = `## üìä Performance Test Results
            
            **Success Rate:** ${report.summary.success_rate.toFixed(2)}%
            **Total Tests:** ${report.summary.total_tests}
            **Regressions:** ${report.regression_analysis.regression_count}
            
            ${report.regression_analysis.has_regressions ? 
              '‚ö†Ô∏è **Performance regressions detected!**' : 
              '‚úÖ No performance regressions detected'}
            `;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
          }

  # Job 5: Security Tests
  security-tests:
    name: Security Tests
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'security' || github.event.inputs.test_type == 'comprehensive' || github.event.inputs.test_type == ''
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements_testing.txt
        pip install bandit safety
    
    - name: Run security tests
      run: |
        python run_security_tests.py
    
    - name: Run Bandit security scan
      run: |
        bandit -r . -f json -o bandit-report.json || true
    
    - name: Run Safety dependency check
      run: |
        safety check --json --output safety-report.json || true
    
    - name: Upload security test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-test-results
        path: |
          *security*.json
          bandit-report.json
          safety-report.json
        retention-days: 90

  # Job 6: Test Analytics and Reporting
  test-analytics:
    name: Test Analytics
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, e2e-tests, performance-tests, security-tests]
    if: always()
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements_testing.txt
        pip install matplotlib seaborn pandas numpy
    
    - name: Download all test results
      uses: actions/download-artifact@v3
      with:
        path: all_test_results/
    
    - name: Generate comprehensive analytics report
      run: |
        python test_analytics.py --import-results all_test_results/ --generate-report --days 30
    
    - name: Upload analytics report
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: test-analytics-report
        path: |
          test_analytics_report_*.html
          test_analytics_report_*.json
        retention-days: 90
    
    - name: Comment analytics summary on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const glob = require('glob');
          
          const reportFiles = glob.sync('test_analytics_report_*.json');
          if (reportFiles.length > 0) {
            const report = JSON.parse(fs.readFileSync(reportFiles[0], 'utf8'));
            
            const qualityEmoji = report.quality_metrics.overall_quality_score >= 90 ? 'üü¢' :
                               report.quality_metrics.overall_quality_score >= 80 ? 'üü°' : 'üî¥';
            
            const comment = `## ${qualityEmoji} Test Quality Report
            
            **Overall Quality Score:** ${report.quality_metrics.overall_quality_score.toFixed(1)}%
            **Success Rate:** ${report.quality_metrics.success_rate.toFixed(1)}%
            **Stability:** ${report.quality_metrics.stability_score.toFixed(1)}%
            **Trend:** ${report.quality_metrics.trend_direction}
            
            **Test Runs:** ${report.summary.total_runs}
            **Total Tests:** ${report.summary.total_tests}
            
            ### Recommendations:
            ${report.recommendations.map(rec => `- ${rec}`).join('\\n')}
            `;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
          }

  # Job 7: Quality Gates
  quality-gates:
    name: Quality Gates
    runs-on: ubuntu-latest
    needs: [test-analytics]
    if: always()
    
    steps:
    - name: Download analytics report
      uses: actions/download-artifact@v3
      with:
        name: test-analytics-report
        path: analytics/
    
    - name: Evaluate quality gates
      run: |
        python -c "
        import json
        import glob
        import sys
        
        report_files = glob.glob('analytics/test_analytics_report_*.json')
        if not report_files:
            print('No analytics report found')
            sys.exit(0)
        
        with open(report_files[0]) as f:
            report = json.load(f)
        
        # Define quality gates
        gates = {
            'min_success_rate': 95,
            'min_quality_score': 80,
            'max_regression_count': 0
        }
        
        # Check gates
        failures = []
        
        if report['quality_metrics']['success_rate'] < gates['min_success_rate']:
            failures.append(f\"Success rate {report['quality_metrics']['success_rate']:.1f}% below {gates['min_success_rate']}%\")
        
        if report['quality_metrics']['overall_quality_score'] < gates['min_quality_score']:
            failures.append(f\"Quality score {report['quality_metrics']['overall_quality_score']:.1f}% below {gates['min_quality_score']}%\")
        
        if 'regression_analysis' in report and report['regression_analysis'].get('regression_count', 0) > gates['max_regression_count']:
            failures.append(f\"Found {report['regression_analysis']['regression_count']} regressions\")
        
        if failures:
            print('Quality gates FAILED:')
            for failure in failures:
                print(f'  - {failure}')
            sys.exit(1)
        else:
            print('All quality gates PASSED')
        "
    
    - name: Set job status
      if: failure()
      run: |
        echo "Quality gates failed. Blocking deployment."
        exit 1

  # Job 8: Deploy Test Reports (on main branch)
  deploy-reports:
    name: Deploy Reports
    runs-on: ubuntu-latest
    needs: [quality-gates]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Download all artifacts
      uses: actions/download-artifact@v3
      with:
        path: reports/
    
    - name: Deploy to GitHub Pages
      uses: peaceiris/actions-gh-pages@v3
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        publish_dir: ./reports
        destination_dir: test-reports/${{ github.run_number }}
    
    - name: Create deployment status
      uses: actions/github-script@v6
      with:
        script: |
          github.rest.repos.createDeploymentStatus({
            owner: context.repo.owner,
            repo: context.repo.repo,
            deployment_id: context.payload.deployment?.id || 0,
            state: 'success',
            environment_url: `https://${context.repo.owner}.github.io/${context.repo.repo}/test-reports/${context.runNumber}`,
            description: 'Test reports deployed successfully'
          });

  # Job 9: Notifications
  notify:
    name: Notifications
    runs-on: ubuntu-latest
    needs: [quality-gates]
    if: always() && (github.ref == 'refs/heads/main' || github.event_name == 'schedule')
    
    steps:
    - name: Send Slack notification
      if: env.SLACK_WEBHOOK_URL != ''
      uses: 8398a7/action-slack@v3
      with:
        status: ${{ job.status }}
        channel: '#voiceflow-testing'
        webhook_url: ${{ secrets.SLACK_WEBHOOK_URL }}
        fields: repo,message,commit,author,action,eventName,ref,workflow
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
    
    - name: Send email notification
      if: failure() && env.EMAIL_NOTIFICATIONS == 'true'
      uses: dawidd6/action-send-mail@v3
      with:
        server_address: smtp.gmail.com
        server_port: 587
        username: ${{ secrets.EMAIL_USERNAME }}
        password: ${{ secrets.EMAIL_PASSWORD }}
        subject: 'VoiceFlow Test Failure - ${{ github.repository }}'
        body: |
          VoiceFlow testing pipeline failed.
          
          Repository: ${{ github.repository }}
          Branch: ${{ github.ref }}
          Commit: ${{ github.sha }}
          Run: ${{ github.run_number }}
          
          Please check the test results and fix any issues.
        to: ${{ secrets.EMAIL_TO }}
        from: VoiceFlow CI/CD